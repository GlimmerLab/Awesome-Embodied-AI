# Multimodal Interaction

> üåê Language: [English](multimodal-interaction_en.md) | [‰∏≠Êñá](multimodal-interaction.md)

This page collects the latest research papers in the field of multimodal interaction for embodied intelligence.

## Main Contents

- Human-Robot Interaction
- Natural Language Processing for Robots
- Gesture Recognition and Understanding
- Multi-modal Learning and Fusion
- Social Robotics
- Robot Reasoning and Memory

## Manually Added Papers

| Date | Title | Paper | Code | Rating |
|------|-------|-------|------|--------|
| 2024-03-20 | MultiModal-GPT: A Vision and Language Model for Robot Interaction | [PDF](https://arxiv.org/abs/2403.13268) | [GitHub](https://github.com/multimodal-robotics/multimodal-gpt) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

## Auto-Updated Papers

*Last updated: 2024-03-21*

| Date | Title | Paper | Code |
|------|-------|-------|------|
| 2024-03-20 | Learning Cross-Modal Representations for Robot Manipulation | [PDF](https://arxiv.org/abs/2403.13269) | [GitHub](https://github.com/cross-modal/robot-learning) |

## Download

- [Download all papers in this category (ZIP)](https://github.com/GlimmerLab/Awesome-Embodied-AI/releases/download/latest/multimodal-interaction-papers.zip)
- [View historical versions](https://github.com/GlimmerLab/Awesome-Embodied-AI/releases)